{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries/packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set up the agnostic code \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Make a binary classification dataset with Scikit-Learn's make_moons() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0087,  0.3682],\n",
      "        [ 0.9214, -0.4969],\n",
      "        [ 0.9402, -0.4982],\n",
      "        [ 0.4659, -0.3454],\n",
      "        [-0.8504,  0.5261]]) tensor([1., 1., 1., 1., 0.])\n",
      "torch.Size([800, 2]) torch.Size([800]) torch.Size([200, 2]) torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(\n",
    "\tn_samples=1000,\n",
    "\trandom_state=42)\n",
    "\n",
    "X,y = torch.from_numpy(X).type(torch.float), torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "print(X[:5], y[:5])\n",
    "\n",
    "train_test_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_test_split], y[:train_test_split]\n",
    "X_test, y_test = X[train_test_split:], y[train_test_split:]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build a model by subclassing nn.Module that incorporates non-linear activation functions and is capable of fitting the data you created in 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MoonModel(\n",
      "  (layer1): Linear(in_features=2, out_features=20, bias=True)\n",
      "  (layer2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (layer3): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import nn\n",
    "from torch import nn\n",
    "\n",
    "# Let's create our model\n",
    "class MoonModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer1 = nn.Linear(in_features=2, out_features=20) # input leyer\n",
    "\t\tself.layer2 = nn.Linear(in_features=20, out_features=20) # hidden layer\n",
    "\t\tself.layer3 = nn.Linear(in_features=20, out_features=1) # output layer\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.layer3(self.relu(self.layer2(self.relu(self.layer1(x)))))\n",
    "\n",
    "model_0 = MoonModel().to(device)\n",
    "print(f\"Model: {model_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Setup a binary classification compatible loss function and optimizer to use when training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1367, 0.0938, 0.1333, 0.0748, 0.1212], grad_fn=<SqueezeBackward0>),\n",
       " tensor([1., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's put some data\n",
    "y_logits = model_0(X_test)[:5]\n",
    "# y_logits.sigmoid()\n",
    "y_logits.squeeze(), y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accuracy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Let's build the training and testing loop \u001b[39;00m\n\u001b[0;32m      5\u001b[0m metric_acc \u001b[38;5;241m=\u001b[39m Accuracy(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBINARY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Let's build the training and testing loop \n",
    "\n",
    "metric_acc = Accuracy(num_classes=2, task='BINARY')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up the epochs\n",
    "epochs = 10001\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\t# Train the model\n",
    "\tmodel_0.train()\n",
    "\n",
    "\t# Forward pass\n",
    "\ty_logits = model_0(X_train).squeeze()\n",
    "\ty_preds = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "\t# Calculate the loss\n",
    "\tloss = loss_fn(y_logits, y_train)\n",
    "\tacc = metric_acc(y_preds, y_train)\n",
    "\t# Optimizer zero grad\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\t# loss backwards\n",
    "\tloss.backward()\n",
    "\n",
    "\t# optimizer step\n",
    "\toptimizer.step()\n",
    "\n",
    "\tmodel_0.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\t# Forward pass\n",
    "\t\ttest_logits = model_0(X_test).squeeze()\n",
    "\t\ttest_preds = torch.round(torch.sigmoid(test_logits))\n",
    "\t\ttest_acc = metric_acc(test_preds, y_test)\n",
    "\t\t# print(test_logits.shape, y_test.shape)\n",
    "\t\t# Calculate the loss\n",
    "\t\ttest_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "\tif epoch % 1000 == 0:\n",
    "\t\tprint(f\"Epoch: {epoch} | Train Loss: {loss}, Accuracy: {(acc*100):.2f}% | Test Loss: {test_loss:.5f}, Accuracy: {(test_acc*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let' plot the data\n",
    "from helper_functions import plot_decision_boundary\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make predictions\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "\ty_logits = model_0(X_test).squeeze()\n",
    "\ty_preds = torch.round(torch.sigmoid(y_logits))\n",
    "\taccuracy_0 = metric_acc(y_logits, y_test)\n",
    "\n",
    "print(f\"Here is the accuracy of the model we've trained: {(accuracy_0 * 100):.2f}%\")\n",
    "print(f\"y_test: {y_test[:5]}\")\n",
    "print(f\"y_logits: {y_logits[:5]}\")\n",
    "print(f\"y_preds: {y_preds[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "\treturn (torch.exp(z) - torch.exp(-z)) / (torch.exp(z) + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(-5, 5, 1, dtype=torch.float32)\n",
    "\n",
    "plt.plot(tanh(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Create a multi-class dataset using the spirals data creation function from CS231n (see below for the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "# Make *X* and *y* tensors\n",
    "\n",
    "X, y = torch.from_numpy(X).type(torch.float32), torch.from_numpy(y).type(torch.LongTensor)\n",
    "\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:5], y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create train and test datasets\n",
    "\n",
    "\n",
    "tt_split = int((len(X) * 0.8))\n",
    "X_train, y_train = X[:tt_split], y[:tt_split]\n",
    "X_test, y_test = X[tt_split:], y[tt_split:]\n",
    "\n",
    "# Print out the shapes\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the model \n",
    "\n",
    "class SpiralModelV0(nn.Module):\n",
    "\tdef __init__(self, input_features, output_features, hidden_units):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear_layers = nn.Sequential(\n",
    "\t\t\tnn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "\t\t\tnn.Tanh(),\n",
    "\t\t\tnn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "\t\t\tnn.Tanh(),\n",
    "\t\t\tnn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.linear_layers(x)\n",
    "\n",
    "# create an instance of model\n",
    "\n",
    "model_1 = SpiralModelV0(input_features=2, output_features=4, hidden_units=10).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a loss function and optimizer \n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a try and use our model\n",
    "\n",
    "y_logits = model_1(X_test.to(device)).squeeze()[:5]\n",
    "y_preds = torch.softmax(y_logits, dim=1)[0]\n",
    "y_logits, y_preds, model_1(X_test.to(device)).squeeze()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let train & test\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "metrics_acc = Accuracy(num_classes=4, task='MULTICLASS').to(device)\n",
    "\n",
    "epochs = 101\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\t# Model train\n",
    "\tmodel_1.train()\n",
    "\n",
    "\t# Forward Pass\n",
    "\ty_logits = model_1(X_train).squeeze()\n",
    "\ty_preds = torch.argmax(torch.softmax(y_logits))\n",
    "\n",
    "\t# Calculate Loss\n",
    "\tloss = loss_fn(y_logits, y_train):.5f\n",
    "\tacc = (metrics_acc(y_preds, y_train) * 100):.2f\n",
    "\n",
    "\t# optimizer zero grad\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\t# loss backwards\n",
    "\tloss.backward()\n",
    "\n",
    "\t# optimizer step\n",
    "\toptimizer.step()\n",
    "\n",
    "\t# Test the model\n",
    "\tmodel.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\t# Forward pass\n",
    "\t\ttest_logits = model_1(X_test).squeeze()\n",
    "\t\ttest_preds = torch.argmax(torch.softmax(test_logits))\n",
    "\n",
    "\t\t# Calculate the loss\n",
    "\t\ttest_loss = loss_fn(y_logits, y_test):.5f\n",
    "\t\ttest_acc = (metrics_acc(y_preds, y_test) * 100):.2f\n",
    "\t\n",
    "\tif epoch %^10 == 0:\n",
    "\t\tprint(f\"Epoch: {epoch} | Train Loss: {loss}, Accuracy: {acc} | Test Loss: {test_loss}, Accuracy: {test_acc}\")\n",
    "\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
